{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5bd8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import hdbscan\n",
    "from matplotlib.lines import Line2D\n",
    "import urllib.request\n",
    "from goatools.obo_parser import GODag\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ff287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Elaborazione Vista: BP ---\n",
      "   Calcolo similarità (può richiedere qualche secondo)...\n",
      "   Matrice completata: (5183, 5183)\n",
      "   -> Salvato in: similarity_BP.csv\n",
      "\n",
      "--- Elaborazione Vista: CC ---\n",
      "   Calcolo similarità (può richiedere qualche secondo)...\n",
      "   Matrice completata: (5183, 5183)\n",
      "   -> Salvato in: similarity_CC.csv\n",
      "\n",
      "--- Elaborazione Vista: MF ---\n",
      "   Calcolo similarità (può richiedere qualche secondo)...\n",
      "   Matrice completata: (5183, 5183)\n",
      "   -> Salvato in: similarity_MF.csv\n",
      "\n",
      "--- Elaborazione Vista: HPO ---\n",
      "   Calcolo similarità (può richiedere qualche secondo)...\n",
      "   Matrice completata: (5183, 5183)\n",
      "   -> Salvato in: similarity_HPO.csv\n",
      "\n",
      "Fase 2 Completata.\n"
     ]
    }
   ],
   "source": [
    "def calculate_tfidf_and_similarity(df, view_name):\n",
    "    print(f\"\\n--- Elaborazione Vista: {view_name} ---\")\n",
    "    n_genes = df.shape[0]\n",
    "    \n",
    "    doc_freq = df.sum(axis=0) \n",
    "    \n",
    "    idf = np.log(n_genes / doc_freq + 1e-10)\n",
    "    \n",
    "    print(\"   Calcolo similarità (può richiedere qualche secondo)...\")\n",
    "    \n",
    "    weighted_matrix = df.values * np.sqrt(idf.values)\n",
    "    \n",
    "    weighted_intersection = np.dot(weighted_matrix, weighted_matrix.T)\n",
    "    \n",
    "    gene_sums = (df * idf).sum(axis=1).values\n",
    "    \n",
    "    weighted_union = gene_sums[:, None] + gene_sums[None, :] - weighted_intersection\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        similarity = weighted_intersection / weighted_union\n",
    "        similarity[np.isnan(similarity)] = 0.0\n",
    "    \n",
    "    np.fill_diagonal(similarity, 1.0)\n",
    "    \n",
    "    sim_df = pd.DataFrame(similarity, index=df.index, columns=df.index)\n",
    "    \n",
    "    print(f\"   Matrice completata: {sim_df.shape}\")\n",
    "    return sim_df\n",
    "\n",
    "input_files = {\n",
    "    \"BP\": \"filtered_BP.csv\",\n",
    "    \"CC\": \"filtered_CC.csv\",\n",
    "    \"MF\": \"filtered_MF.csv\",\n",
    "    \"HPO\": \"filtered_HPO.csv\" \n",
    "}\n",
    "\n",
    "similarity_results = {}\n",
    "\n",
    "for key, filename in input_files.items():\n",
    "    if os.path.exists(filename):\n",
    "        # Carica dati\n",
    "        df = pd.read_csv(filename, index_col=0)\n",
    "        \n",
    "        # Calcola\n",
    "        sim_matrix = calculate_tfidf_and_similarity(df, key)\n",
    "        \n",
    "        # Salva\n",
    "        output_file = f\"similarity_{key}.csv\"\n",
    "        sim_matrix.to_csv(output_file)\n",
    "        print(f\"   -> Salvato in: {output_file}\")\n",
    "        \n",
    "        similarity_results[key] = sim_matrix\n",
    "    else:\n",
    "        print(f\"ATTENZIONE: File {filename} non trovato. Hai eseguito la Fase 1?\")\n",
    "\n",
    "print(\"\\nFase 2 Completata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_view(sim_matrix_path, view_name):\n",
    "    print(f\"\\n--- Analisi Vista: {view_name} ---\")\n",
    "    \n",
    "    sim_df = pd.read_csv(sim_matrix_path, index_col=0)\n",
    "    \n",
    "    distance_matrix = 1 - sim_df.values\n",
    "    distance_matrix[distance_matrix < 0] = 0\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        min_dist=0.1,\n",
    "        n_components=2,\n",
    "        metric='precomputed',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(distance_matrix)\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=30,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    \n",
    "    cluster_labels = clusterer.fit_predict(embedding)\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    print(f\"   -> Trovati {n_clusters} cluster.\")\n",
    "    print(f\"   -> Geni scartati come rumore: {n_noise}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Definizione colori\n",
    "    noise_color = (0.8, 0.8, 0.8)\n",
    "    palette = sns.color_palette('tab20', n_colors=n_clusters)\n",
    "    cluster_colors = [palette[x] if x >= 0 else noise_color for x in cluster_labels]\n",
    "    \n",
    "    plt.scatter(\n",
    "        embedding[:, 0], \n",
    "        embedding[:, 1], \n",
    "        c=cluster_colors, \n",
    "        s=5, \n",
    "        alpha=0.6\n",
    "    )\n",
    "    \n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Noise', \n",
    "               markerfacecolor=noise_color, markersize=10),\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.title(f'UMAP Projection - {view_name} ({n_clusters} clusters)', fontsize=16)\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    \n",
    "    plt.savefig(f\"plot_umap_{view_name}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Gene': sim_df.index,\n",
    "        'Cluster': cluster_labels,\n",
    "        'UMAP_1': embedding[:, 0],\n",
    "        'UMAP_2': embedding[:, 1]\n",
    "    })\n",
    "    results.to_csv(f\"clusters_{view_name}.csv\", index=False)\n",
    "    \n",
    "    return n_clusters, n_noise\n",
    "\n",
    "files = {\n",
    "    \"BP\": \"similarity_BP.csv\",\n",
    "    \"CC\": \"similarity_CC.csv\",\n",
    "    \"MF\": \"similarity_MF.csv\",\n",
    "    \"HPO\": \"similarity_HPO.csv\"\n",
    "}\n",
    "\n",
    "summary = []\n",
    "for name, path in files.items():\n",
    "    n_clust, n_noise = analyze_view(path, name)\n",
    "    summary.append({'View': name, 'Clusters': n_clust, 'Noise_Genes': n_noise})\n",
    "\n",
    "print(pd.DataFrame(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02846734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go-basic.obo: fmt(1.2) rel(2025-10-10) 42,666 Terms\n",
      "hp.obo: fmt(1.2) rel(hp/2025-11-24) 23,288 Terms\n",
      "      Coppie simili trovate nel DAG: 1085\n",
      "      Termini ridondanti (Antenati) rimossi: 753\n",
      "      Coppie simili trovate nel DAG: 66\n",
      "      Termini ridondanti (Antenati) rimossi: 36\n",
      "      Coppie simili trovate nel DAG: 194\n",
      "      Termini ridondanti (Antenati) rimossi: 122\n",
      "      Coppie simili trovate nel DAG: 9066\n",
      "      Termini ridondanti (Antenati) rimossi: 833\n",
      "--- Filtro Geni Comuni Attivi ---\n",
      "   Vista BP: 5011 geni attivi su 5183\n",
      "   Vista CC: 4221 geni attivi su 5183\n",
      "   Vista MF: 4034 geni attivi su 5183\n",
      "   Vista HPO: 5162 geni attivi su 5183\n",
      "   -> Geni validi comuni rimasti: 3317\n",
      "\n",
      "=== Salvataggio ===\n",
      "   -> BP: (3317, 4807) salvato in filtered_final_BP.csv\n",
      "   -> CC: (3317, 461) salvato in filtered_final_CC.csv\n",
      "   -> MF: (3317, 911) salvato in filtered_final_MF.csv\n",
      "   -> HPO: (3317, 6038) salvato in filtered_final_HPO.csv\n"
     ]
    }
   ],
   "source": [
    "obo_files = {\n",
    "    \"GO\": {\"url\": \"https://current.geneontology.org/ontology/go-basic.obo\", \"path\": \"go-basic.obo\"},\n",
    "    \"HPO\": {\"url\": \"https://raw.githubusercontent.com/obophenotype/human-phenotype-ontology/master/hp.obo\", \"path\": \"hp.obo\"}\n",
    "}\n",
    "\n",
    "data_files = {\n",
    "    \"BP\": r\"C:\\Users\\nicki\\Desktop\\magi\\Anno 1\\Q1\\ScientificVisualization\\csv_data\\gene_go_matrix_propT_rel-is_a-part_of_ont-BP.csv\",\n",
    "    \"CC\": r\"C:\\Users\\nicki\\Desktop\\magi\\Anno 1\\Q1\\ScientificVisualization\\csv_data\\gene_go_matrix_propT_rel-is_a-part_of_ont-CC.csv\",\n",
    "    \"MF\": r\"C:\\Users\\nicki\\Desktop\\magi\\Anno 1\\Q1\\ScientificVisualization\\csv_data\\gene_go_matrix_propT_rel-is_a-part_of_ont-MF.csv\",\n",
    "    \"HPO\": r\"C:\\Users\\nicki\\Desktop\\magi\\Anno 1\\Q1\\ScientificVisualization\\csv_data\\gene_hpo_matrix_binary_withAncestors_namespace_Phenotypic_abnormality.csv\"\n",
    "}\n",
    "\n",
    "depth_files = {\n",
    "    \"BP\": \"./csv_data/goterm_depth_propT_rel-is_a-part_of_ont-BP.csv\",\n",
    "    \"CC\": \"./csv_data/goterm_depth_propT_rel-is_a-part_of_ont-CC.csv\",\n",
    "    \"MF\": \"./csv_data/goterm_depth_propT_rel-is_a-part_of_ont-MF.csv\"\n",
    "}\n",
    "\n",
    "def check_and_download(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "for key, info in obo_files.items():\n",
    "    check_and_download(info[\"url\"], info[\"path\"])\n",
    "\n",
    "\n",
    "def load_ontology(obo_path):\n",
    "    try:\n",
    "        return GODag(obo_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def normalize_id(term_id):\n",
    "    term_id = str(term_id)\n",
    "    if \"GO\" in term_id or \"HP\" in term_id:\n",
    "        return term_id.replace(\".\", \":\").replace(\"_\", \":\")\n",
    "    return term_id\n",
    "\n",
    "def filter_by_depth(df, depth_file, min_depth=4):\n",
    "    \n",
    "    try:\n",
    "        df_depth = pd.read_csv(depth_file, index_col=0, nrows=1)\n",
    "        depth_series = pd.to_numeric(df_depth.iloc[0], errors='coerce').dropna()\n",
    "        valid_terms = set(depth_series[depth_series >= min_depth].index)\n",
    "        \n",
    "        cols_to_keep = []\n",
    "        for col in df.columns:\n",
    "            if col in valid_terms or normalize_id(col) in valid_terms:\n",
    "                cols_to_keep.append(col)\n",
    "                \n",
    "        return df[cols_to_keep]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return df\n",
    "\n",
    "def frequency_filtering(df, min_genes=3, max_pct=0.20):\n",
    "    counts = df.sum(axis=0)\n",
    "    limit = df.shape[0] * max_pct\n",
    "    mask = (counts >= min_genes) & (counts <= limit)\n",
    "    df_filtered = df.loc[:, mask]\n",
    "    return df_filtered\n",
    "\n",
    "def remove_semantic_redundancy(df, dag, threshold=0.7):\n",
    "\n",
    "    example_col = df.columns[0]\n",
    "    normalized_ex = normalize_id(example_col)\n",
    "    if normalized_ex not in dag:\n",
    "        print(f\"Il termine '{example_col}' (norm: '{normalized_ex}') non è stato trovato nel DAG!\")\n",
    "\n",
    "\n",
    "    matrix = (df.values > 0).astype(int).T\n",
    "    intersect = matrix @ matrix.T\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    union = row_sums[:, None] + row_sums[None, :] - intersect\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        sim_matrix = np.triu(intersect / union, k=1)\n",
    "\n",
    "    pairs = np.where(sim_matrix >= threshold)\n",
    "    to_drop = set()\n",
    "    cols = df.columns\n",
    "    \n",
    "    match_count = 0\n",
    "    \n",
    "    for i, j in zip(*pairs):\n",
    "        term_a_raw = cols[i]\n",
    "        term_b_raw = cols[j]\n",
    "        \n",
    "        term_a = normalize_id(term_a_raw)\n",
    "        term_b = normalize_id(term_b_raw)\n",
    "        \n",
    "        if term_a in dag and term_b in dag:\n",
    "            match_count += 1\n",
    "            parents_b = dag[term_b].get_all_parents()\n",
    "            parents_a = dag[term_a].get_all_parents()\n",
    "            \n",
    "            if term_a in parents_b:\n",
    "                to_drop.add(term_a_raw)\n",
    "            elif term_b in parents_a:\n",
    "                to_drop.add(term_b_raw)\n",
    "            \n",
    "    return df.drop(columns=list(to_drop))\n",
    "\n",
    "def keep_common_active_genes(dfs_dict):\n",
    "    print(\"--- Filtro Geni Comuni Attivi ---\")\n",
    "    \n",
    "    # 1. Trova i geni che hanno almeno un valore != 0 in OGNI vista\n",
    "    valid_genes_per_view = []\n",
    "    \n",
    "    for name, df in dfs_dict.items():\n",
    "        # Calcola la somma per riga (gene)\n",
    "        row_sums = df.sum(axis=1)\n",
    "        # Tieni solo i geni con somma > 0\n",
    "        active_genes = set(row_sums[row_sums > 0].index)\n",
    "        valid_genes_per_view.append(active_genes)\n",
    "        print(f\"   Vista {name}: {len(active_genes)} geni attivi su {len(df)}\")\n",
    "\n",
    "    common_genes = set.intersection(*valid_genes_per_view)\n",
    "    common_genes = sorted(list(common_genes))\n",
    "    \n",
    "    print(f\"   -> Geni validi comuni rimasti: {len(common_genes)}\")\n",
    "    \n",
    "    filtered_dict = {}\n",
    "    for name, df in dfs_dict.items():\n",
    "        # .loc seleziona solo le righe dei geni comuni\n",
    "        filtered_dict[name] = df.loc[common_genes]\n",
    "        \n",
    "    return filtered_dict\n",
    "\n",
    "go_dag = load_ontology(obo_files[\"GO\"][\"path\"])\n",
    "hpo_dag = load_ontology(obo_files[\"HPO\"][\"path\"])\n",
    "\n",
    "processed_dfs = {}\n",
    "\n",
    "for key, path in data_files.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        \n",
    "        if key in depth_files:\n",
    "            df = filter_by_depth(df, depth_files[key], min_depth=4)\n",
    "        df = frequency_filtering(df)\n",
    "        \n",
    "        current_dag = hpo_dag if key == \"HPO\" else go_dag\n",
    "        \n",
    "        df = remove_semantic_redundancy(df, current_dag, threshold=0.7)\n",
    "        \n",
    "        processed_dfs[key] = df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File non trovato: {path}\")\n",
    "\n",
    "if processed_dfs:\n",
    "    final_dfs = keep_common_active_genes(processed_dfs)\n",
    "    print(\"\\n=== Salvataggio ===\")\n",
    "    for key, df in final_dfs.items():\n",
    "        out_name = f\"filtered_final_{key}.csv\"\n",
    "        df.to_csv(out_name)\n",
    "        print(f\"   -> {key}: {df.shape} salvato in {out_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614912b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Elaborazione Vista: BP ---\n",
      "   Input (Binario): (3317, 4807)\n",
      "   TF-IDF Calcolato. Range valori: [0.0000, 1.0000]\n",
      "   Similarità Calcolata: (3317, 3317). Media diagonale: 1.00 (atteso 1.0)\n",
      "\n",
      "--- Elaborazione Vista: CC ---\n",
      "   Input (Binario): (3317, 461)\n",
      "   TF-IDF Calcolato. Range valori: [0.0000, 1.0000]\n",
      "   Similarità Calcolata: (3317, 3317). Media diagonale: 1.00 (atteso 1.0)\n",
      "\n",
      "--- Elaborazione Vista: MF ---\n",
      "   Input (Binario): (3317, 911)\n",
      "   TF-IDF Calcolato. Range valori: [0.0000, 1.0000]\n",
      "   Similarità Calcolata: (3317, 3317). Media diagonale: 1.00 (atteso 1.0)\n",
      "\n",
      "--- Elaborazione Vista: HPO ---\n",
      "   Input (Binario): (3317, 6038)\n",
      "   TF-IDF Calcolato. Range valori: [0.0000, 1.0000]\n",
      "   Similarità Calcolata: (3317, 3317). Media diagonale: 1.00 (atteso 1.0)\n"
     ]
    }
   ],
   "source": [
    "input_files = {\n",
    "    \"BP\": \"filtered_final_BP.csv\",\n",
    "    \"CC\": \"filtered_final_CC.csv\",\n",
    "    \"MF\": \"filtered_final_MF.csv\",\n",
    "    \"HPO\": \"filtered_final_HPO.csv\"\n",
    "}\n",
    "\n",
    "def process_phase2(file_path, view_name):\n",
    "    print(f\"\\n--- Elaborazione Vista: {view_name} ---\")\n",
    "    \n",
    "    # 1. Caricamento Dati (Matrice Binaria)\n",
    "    try:\n",
    "        df_binary = pd.read_csv(file_path, index_col=0)\n",
    "        print(f\"   Input (Binario): {df_binary.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ERRORE: File {file_path} non trovato. Salto.\")\n",
    "        return\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "    \n",
    "    tfidf_matrix_sparse = tfidf_transformer.fit_transform(df_binary)\n",
    "    \n",
    "    df_tfidf = pd.DataFrame(\n",
    "        tfidf_matrix_sparse.toarray(), \n",
    "        index=df_binary.index, \n",
    "        columns=df_binary.columns\n",
    "    )\n",
    "    print(f\"   TF-IDF Calcolato. Range valori: [{df_tfidf.values.min():.4f}, {df_tfidf.values.max():.4f}]\")\n",
    "\n",
    "    # 3. Output per MOFA (Gene x Termini, Pesato)\n",
    "    mofa_filename = f\"mofa_input_{view_name}.csv\"\n",
    "    df_tfidf.to_csv(mofa_filename)\n",
    "\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix_sparse)\n",
    "    \n",
    "    # Ricostruzione DataFrame simmetrico Gene x Gene\n",
    "    df_sim = pd.DataFrame(\n",
    "        cosine_sim,\n",
    "        index=df_binary.index,\n",
    "        columns=df_binary.index\n",
    "    )\n",
    "    \n",
    "    # Check integrità (Diagonale deve essere 1.0)\n",
    "    diag_mean = np.diag(df_sim).mean()\n",
    "    print(f\"   Similarità Calcolata: {df_sim.shape}. Media diagonale: {diag_mean:.2f} (atteso 1.0)\")\n",
    "\n",
    "    snf_filename = f\"snf_similarity_{view_name}.csv\"\n",
    "    df_sim.to_csv(snf_filename)\n",
    "\n",
    "for key, filename in input_files.items():\n",
    "    process_phase2(filename, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0796fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analisi Vista: BP ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1865: UserWarning: using precomputed metric; inverse_transform will be unavailable\n",
      "  warn(\"using precomputed metric; inverse_transform will be unavailable\")\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Trovati 23 cluster.\n",
      "   -> Geni scartati come rumore: 1632\n",
      "\n",
      "--- Analisi Vista: CC ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1865: UserWarning: using precomputed metric; inverse_transform will be unavailable\n",
      "  warn(\"using precomputed metric; inverse_transform will be unavailable\")\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Trovati 31 cluster.\n",
      "   -> Geni scartati come rumore: 992\n",
      "\n",
      "--- Analisi Vista: MF ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1865: UserWarning: using precomputed metric; inverse_transform will be unavailable\n",
      "  warn(\"using precomputed metric; inverse_transform will be unavailable\")\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Trovati 34 cluster.\n",
      "   -> Geni scartati come rumore: 838\n",
      "\n",
      "--- Analisi Vista: HPO ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1865: UserWarning: using precomputed metric; inverse_transform will be unavailable\n",
      "  warn(\"using precomputed metric; inverse_transform will be unavailable\")\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nicki\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Trovati 21 cluster.\n",
      "   -> Geni scartati come rumore: 959\n",
      "  View  Clusters  Noise_Genes\n",
      "0   BP        23         1632\n",
      "1   CC        31          992\n",
      "2   MF        34          838\n",
      "3  HPO        21          959\n"
     ]
    }
   ],
   "source": [
    "def analyze_view(sim_matrix_path, view_name):\n",
    "    print(f\"\\n--- Analisi Vista: {view_name} ---\")\n",
    "    \n",
    "    sim_df = pd.read_csv(sim_matrix_path, index_col=0)\n",
    "    \n",
    "    distance_matrix = 1 - sim_df.values\n",
    "    distance_matrix[distance_matrix < 0] = 0\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        min_dist=0.1,\n",
    "        n_components=2,\n",
    "        metric='precomputed',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(distance_matrix)\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=30,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    \n",
    "    cluster_labels = clusterer.fit_predict(embedding)\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    print(f\"   -> Trovati {n_clusters} cluster.\")\n",
    "    print(f\"   -> Geni scartati come rumore: {n_noise}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Definizione colori\n",
    "    noise_color = (0.8, 0.8, 0.8)\n",
    "    palette = sns.color_palette('tab20', n_colors=n_clusters)\n",
    "    cluster_colors = [palette[x] if x >= 0 else noise_color for x in cluster_labels]\n",
    "    \n",
    "    plt.scatter(\n",
    "        embedding[:, 0], \n",
    "        embedding[:, 1], \n",
    "        c=cluster_colors, \n",
    "        s=5, \n",
    "        alpha=0.6\n",
    "    )\n",
    "    \n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Noise', \n",
    "               markerfacecolor=noise_color, markersize=10),\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.title(f'UMAP Projection - {view_name} ({n_clusters} clusters)', fontsize=16)\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    \n",
    "    plt.savefig(f\"plot_umap_{view_name}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Gene': sim_df.index,\n",
    "        'Cluster': cluster_labels,\n",
    "        'UMAP_1': embedding[:, 0],\n",
    "        'UMAP_2': embedding[:, 1]\n",
    "    })\n",
    "    results.to_csv(f\"clusters_{view_name}.csv\", index=False)\n",
    "    \n",
    "    return n_clusters, n_noise\n",
    "\n",
    "files = {\n",
    "    \"BP\": \"snf_similarity_BP.csv\",\n",
    "    \"CC\": \"snf_similarity_CC.csv\",\n",
    "    \"MF\": \"snf_similarity_MF.csv\",\n",
    "    \"HPO\": \"snf_similarity_HPO.csv\"\n",
    "}\n",
    "\n",
    "summary = []\n",
    "for name, path in files.items():\n",
    "    n_clust, n_noise = analyze_view(path, name)\n",
    "    summary.append({'View': name, 'Clusters': n_clust, 'Noise_Genes': n_noise})\n",
    "\n",
    "print(pd.DataFrame(summary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
