{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c9f6d6",
   "metadata": {},
   "source": [
    "# üß¨ Pipeline di Integrazione Dati Genici \"Semantic-Aware\" (SNF + MOFA+)\n",
    "\n",
    "Questa pipeline √® progettata per l'integrazione di dati funzionali (Gene Ontology, HPO) al fine di identificare cluster di geni funzionalmente correlati. Adotta una duplice strategia, combinando la topologia locale di **SNF (Similarity Network Fusion)** con la scomposizione della varianza globale di **MOFA+ (Multi-Omics Factor Analysis)**, culminando in una proiezione UMAP guidata per un clustering robusto e semanticamente ricco.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Fase: Pre-processing \"Semantic-Aware\" üßπ\n",
    "\n",
    "**Obiettivo:** Ridurre il rumore e la ridondanza semantica nelle matrici funzionali, mantenendo la pertinenza biologica.\n",
    "\n",
    "### 1.1. Filtraggio Iniziale per Frequenza\n",
    "* **Termini Rari:** Eliminare i termini funzionali associati a meno di **3 geni** (troppo rari).\n",
    "* **Termini Generici:** Eliminare i termini associati a pi√π del **20%** del totale dei geni (troppo generici).\n",
    "\n",
    "### 1.2. Semantic Redundancy Removal\n",
    "Si sfrutta la struttura del **Grafo Aciclico Diretto (DAG)** di GO/HPO per calcolare la **Similarit√† Semantica** (es. Wang o Resnik).\n",
    "\n",
    "**Algoritmo:** Se $Sim(t_1, t_2) > 0.7$ **E** $t_1$ √® antenato di $t_2$ (pi√π generale), allora **scartare $t_1$** (mantenendo il termine $t_2$, che √® pi√π specifico/informativo).\n",
    "\n",
    "### 1.3. Gestione Geni Orfani\n",
    "Rimuovere i geni che, dopo il filtraggio dei termini, risultano avere vettori $[0,0,...,0]$ in tutte le viste.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Fase: Feature Engineering & Trasformazione ‚öôÔ∏è\n",
    "\n",
    "**Obiettivo:** Preparare le matrici di input (Geni $\\times$ Termini) per le strategie parallele SNF e MOFA.\n",
    "\n",
    "### 2.1. Calcolo TF-IDF\n",
    "Applicare la metrica **Term Frequency-Inverse Document Frequency (TF-IDF)** alle matrici binarie iniziali.\n",
    "\n",
    "$$W_{i,j} = tf_{i,j} \\times \\log\\left(\\frac{N}{df_j}\\right)$$\n",
    "\n",
    "### 2.2. Creazione delle Matrici di Input\n",
    "\n",
    "* **Per MOFA+:** Mantenere le matrici **TF-IDF** (Gene $\\times$ Termini) come input diretto (\"Views\").\n",
    "* **Per SNF:** Calcolare le matrici di **Similarit√†** (Gene $\\times$ Gene) usando **Cosine Similarity** sulle matrici TF-IDF.\n",
    "\n",
    "$$S_{view} = \\frac{X_{tfidf} \\cdot X_{tfidf}^T}{||X_{tfidf}|| \\cdot ||X_{tfidf}||^T}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Fase: Dual Integration Strategy (SNF & MOFA+) ü§ù\n",
    "\n",
    "### Strada A: Similarity Network Fusion (SNF) - Topologia Locale\n",
    "**Obiettivo:** Fondere le similarit√† amplificando i legami forti.\n",
    "* **Gestione HPO:** Includere $S_{HPO}$. SNF gestisce la sparsit√† penalizzando naturalmente le viste rumorose.\n",
    "* **Parametri:** $K$ dinamico (es. 20), $t=20$ iterazioni.\n",
    "* **Output A:** Matrice fusa $W_{fused}$ (Gene $\\times$ Gene). \n",
    "\n",
    "### Strada B: MOFA+ (Multi-Omics Factor Analysis) - Varianza Globale\n",
    "**Obiettivo:** Estrarre fattori latenti che spiegano la variabilit√†.\n",
    "* **Input:** Le 4 matrici TF-IDF (CC, MF, BP, HPO) come Views.\n",
    "* **Processo:** Modellazione Bayesiana per fattorizzare le matrici: $Y_{view} = Z \\cdot W_{view}^T + \\epsilon$.\n",
    "* **Output B:** Matrice dei Fattori Latenti $Z_{MOFA}$ (Geni $\\times$ Fattori), es. 5183 $\\times$ 15.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Fase: Integrazione Dimensionale & Proiezione (UMAP Guidato) üó∫Ô∏è\n",
    "\n",
    "**Obiettivo:** Unire la topologia locale di SNF con la spiegabilit√† globale di MOFA per la proiezione finale.\n",
    "\n",
    "Si usa **UMAP** con un'inizializzazione guidata:\n",
    "* **Metrica:** Usare la distanza derivata da SNF ($D = 1 - W_{fused}$) per definire la topologia locale.\n",
    "* **Inizializzazione:** Usare i primi $N$ fattori di MOFA ($Z_{MOFA}$) come punto di partenza per l'embedding, guidando la struttura globale.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Fase: Clustering Robusto (Soft Clustering) üìä\n",
    "\n",
    "**Obiettivo:** Identificare gruppi di geni funzionalmente correlati senza forzare il rumore.\n",
    "\n",
    "### 5.1. HDBSCAN\n",
    "\n",
    "Applicare HDBSCAN all'embedding UMAP generato\n",
    "* **Parametri:** min_cluster_size=30, min_samples=10.\n",
    "\n",
    "### 5.2. Recupero Rumore (Soft Clustering)\n",
    "Generare vettori di probabilit√† per ogni gene ($P(gene \\in Cluster_i)$).\n",
    "\n",
    "**Regola:** Se un gene √® classificato come **-1 (rumore)** ma ha una probabilit√† di appartenenza ($\\lambda$) per un Cluster X **maggiore di $0.3$**, riassegnalo al Cluster X.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Fase: Validazione & Explainability (Biologica) üß™\n",
    "\n",
    "### A. Enrichment Analysis (Validazione Esterna)\n",
    "\n",
    "Per ogni cluster, eseguire test ipergeometrici su **KEGG/Reactome** (FDR < 0.05) per convalidare la coerenza funzionale.\n",
    "\n",
    "### B. MOFA Factor Characterization (Validazione Interna)\n",
    "Usare i pesi dei fattori di MOFA ($W_{view}$) per capire i driver della separazione:\n",
    "\n",
    "* Analizzare quali viste (BP, CC, MF, HPO) pesano di pi√π sui Fattori Latenti che separano i cluster.\n",
    "* **Esempio:** \"Il Cluster 1 e 2 sono separati lungo il Fattore 1, che √® guidato al 80% dalla vista HPO (termini legati a dismorfismi).\"\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2f97f",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5bd8afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import hdbscan\n",
    "from matplotlib.lines import Line2D\n",
    "import urllib.request\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from snf import snf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ff287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf_and_similarity(df, view_name):\n",
    "    print(f\"\\n--- Elaborazione Vista: {view_name} ---\")\n",
    "    n_genes = df.shape[0]\n",
    "    \n",
    "    doc_freq = df.sum(axis=0) \n",
    "    \n",
    "    idf = np.log(n_genes / doc_freq + 1e-10)\n",
    "    \n",
    "    print(\"   Calcolo similarit√† (pu√≤ richiedere qualche secondo)...\")\n",
    "    \n",
    "    weighted_matrix = df.values * np.sqrt(idf.values)\n",
    "    \n",
    "    weighted_intersection = np.dot(weighted_matrix, weighted_matrix.T)\n",
    "    \n",
    "    gene_sums = (df * idf).sum(axis=1).values\n",
    "    \n",
    "    weighted_union = gene_sums[:, None] + gene_sums[None, :] - weighted_intersection\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        similarity = weighted_intersection / weighted_union\n",
    "        similarity[np.isnan(similarity)] = 0.0\n",
    "    \n",
    "    np.fill_diagonal(similarity, 1.0)\n",
    "    \n",
    "    sim_df = pd.DataFrame(similarity, index=df.index, columns=df.index)\n",
    "    \n",
    "    print(f\"   Matrice completata: {sim_df.shape}\")\n",
    "    return sim_df\n",
    "\n",
    "input_files = {\n",
    "    \"BP\": \"filtered_BP.csv\",\n",
    "    \"CC\": \"filtered_CC.csv\",\n",
    "    \"MF\": \"filtered_MF.csv\",\n",
    "    \"HPO\": \"filtered_HPO.csv\" \n",
    "}\n",
    "\n",
    "similarity_results = {}\n",
    "\n",
    "for key, filename in input_files.items():\n",
    "    if os.path.exists(filename):\n",
    "        # Carica dati\n",
    "        df = pd.read_csv(filename, index_col=0)\n",
    "        \n",
    "        # Calcola\n",
    "        sim_matrix = calculate_tfidf_and_similarity(df, key)\n",
    "        \n",
    "        # Salva\n",
    "        output_file = f\"similarity_{key}.csv\"\n",
    "        sim_matrix.to_csv(output_file)\n",
    "        print(f\"   -> Salvato in: {output_file}\")\n",
    "        \n",
    "        similarity_results[key] = sim_matrix\n",
    "    else:\n",
    "        print(f\"ATTENZIONE: File {filename} non trovato. Hai eseguito la Fase 1?\")\n",
    "\n",
    "print(\"\\nFase 2 Completata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_view(sim_matrix_path, view_name):\n",
    "    print(f\"\\n--- Analisi Vista: {view_name} ---\")\n",
    "    \n",
    "    sim_df = pd.read_csv(sim_matrix_path, index_col=0)\n",
    "    \n",
    "    distance_matrix = 1 - sim_df.values\n",
    "    distance_matrix[distance_matrix < 0] = 0\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        min_dist=0.1,\n",
    "        n_components=2,\n",
    "        metric='precomputed',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(distance_matrix)\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=30,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    \n",
    "    cluster_labels = clusterer.fit_predict(embedding)\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    print(f\"   -> Trovati {n_clusters} cluster.\")\n",
    "    print(f\"   -> Geni scartati come rumore: {n_noise}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Definizione colori\n",
    "    noise_color = (0.8, 0.8, 0.8)\n",
    "    palette = sns.color_palette('tab20', n_colors=n_clusters)\n",
    "    cluster_colors = [palette[x] if x >= 0 else noise_color for x in cluster_labels]\n",
    "    \n",
    "    plt.scatter(\n",
    "        embedding[:, 0], \n",
    "        embedding[:, 1], \n",
    "        c=cluster_colors, \n",
    "        s=5, \n",
    "        alpha=0.6\n",
    "    )\n",
    "    \n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Noise', \n",
    "               markerfacecolor=noise_color, markersize=10),\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.title(f'UMAP Projection - {view_name} ({n_clusters} clusters)', fontsize=16)\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    \n",
    "    plt.savefig(f\"plot_umap_{view_name}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Gene': sim_df.index,\n",
    "        'Cluster': cluster_labels,\n",
    "        'UMAP_1': embedding[:, 0],\n",
    "        'UMAP_2': embedding[:, 1]\n",
    "    })\n",
    "    results.to_csv(f\"clusters_{view_name}.csv\", index=False)\n",
    "    \n",
    "    return n_clusters, n_noise\n",
    "\n",
    "files = {\n",
    "    \"BP\": \"similarity_BP.csv\",\n",
    "    \"CC\": \"similarity_CC.csv\",\n",
    "    \"MF\": \"similarity_MF.csv\",\n",
    "    \"HPO\": \"similarity_HPO.csv\"\n",
    "}\n",
    "\n",
    "summary = []\n",
    "for name, path in files.items():\n",
    "    n_clust, n_noise = analyze_view(path, name)\n",
    "    summary.append({'View': name, 'Clusters': n_clust, 'Noise_Genes': n_noise})\n",
    "\n",
    "print(pd.DataFrame(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac7ce4",
   "metadata": {},
   "source": [
    "### STARTING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02846734",
   "metadata": {},
   "outputs": [],
   "source": [
    "obo_files = {\n",
    "    \"GO\": {\"url\": \"https://current.geneontology.org/ontology/go-basic.obo\", \"path\": \"go-basic.obo\"},\n",
    "    \"HPO\": {\"url\": \"https://raw.githubusercontent.com/obophenotype/human-phenotype-ontology/master/hp.obo\", \"path\": \"hp.obo\"}\n",
    "}\n",
    "\n",
    "data_files = {\n",
    "    \"BP\": r\"C:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\csv_data\\gene_go_matrix_propT_rel-is_a-part_of_ont-BP.csv\",\n",
    "    \"CC\": r\"C:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\csv_data\\gene_go_matrix_propT_rel-is_a-part_of_ont-CC.csv\",\n",
    "    \"MF\": r\"C:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\csv_data\\gene_go_matrix_propT_rel-is_a-part_of_ont-MF.csv\",\n",
    "    \"HPO\": r\"C:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\csv_data\\gene_hpo_matrix_binary_withAncestors_namespace_Phenotypic_abnormality.csv\"\n",
    "}\n",
    "\n",
    "depth_files = {\n",
    "    \"BP\": \"./csv_data/goterm_depth_propT_rel-is_a-part_of_ont-BP.csv\",\n",
    "    \"CC\": \"./csv_data/goterm_depth_propT_rel-is_a-part_of_ont-CC.csv\",\n",
    "    \"MF\": \"./csv_data/goterm_depth_propT_rel-is_a-part_of_ont-MF.csv\"\n",
    "}\n",
    "\n",
    "def check_and_download(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "for key, info in obo_files.items():\n",
    "    check_and_download(info[\"url\"], info[\"path\"])\n",
    "\n",
    "\n",
    "def load_ontology(obo_path):\n",
    "    try:\n",
    "        return GODag(obo_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def normalize_id(term_id):\n",
    "    term_id = str(term_id)\n",
    "    if \"GO\" in term_id or \"HP\" in term_id:\n",
    "        return term_id.replace(\".\", \":\").replace(\"_\", \":\")\n",
    "    return term_id\n",
    "\n",
    "def filter_by_depth(df, depth_file, min_depth=4):\n",
    "    \n",
    "    try:\n",
    "        df_depth = pd.read_csv(depth_file, index_col=0, nrows=1)\n",
    "        depth_series = pd.to_numeric(df_depth.iloc[0], errors='coerce').dropna()\n",
    "        valid_terms = set(depth_series[depth_series >= min_depth].index)\n",
    "        \n",
    "        cols_to_keep = []\n",
    "        for col in df.columns:\n",
    "            if col in valid_terms or normalize_id(col) in valid_terms:\n",
    "                cols_to_keep.append(col)\n",
    "                \n",
    "        return df[cols_to_keep]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return df\n",
    "\n",
    "def frequency_filtering(df, min_genes=3, max_pct=0.20):\n",
    "    counts = df.sum(axis=0)\n",
    "    limit = df.shape[0] * max_pct\n",
    "    mask = (counts >= min_genes) & (counts <= limit)\n",
    "    df_filtered = df.loc[:, mask]\n",
    "    return df_filtered\n",
    "\n",
    "def remove_semantic_redundancy(df, dag, threshold=0.7):\n",
    "\n",
    "    example_col = df.columns[0]\n",
    "    normalized_ex = normalize_id(example_col)\n",
    "    if normalized_ex not in dag:\n",
    "        print(f\"Il termine '{example_col}' (norm: '{normalized_ex}') non √® stato trovato nel DAG!\")\n",
    "\n",
    "\n",
    "    matrix = (df.values > 0).astype(int).T\n",
    "    intersect = matrix @ matrix.T\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    union = row_sums[:, None] + row_sums[None, :] - intersect\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        sim_matrix = np.triu(intersect / union, k=1)\n",
    "\n",
    "    pairs = np.where(sim_matrix >= threshold)\n",
    "    to_drop = set()\n",
    "    cols = df.columns\n",
    "    \n",
    "    match_count = 0\n",
    "    \n",
    "    for i, j in zip(*pairs):\n",
    "        term_a_raw = cols[i]\n",
    "        term_b_raw = cols[j]\n",
    "        \n",
    "        term_a = normalize_id(term_a_raw)\n",
    "        term_b = normalize_id(term_b_raw)\n",
    "        \n",
    "        if term_a in dag and term_b in dag:\n",
    "            match_count += 1\n",
    "            parents_b = dag[term_b].get_all_parents()\n",
    "            parents_a = dag[term_a].get_all_parents()\n",
    "            \n",
    "            if term_a in parents_b:\n",
    "                to_drop.add(term_a_raw)\n",
    "            elif term_b in parents_a:\n",
    "                to_drop.add(term_b_raw)\n",
    "            \n",
    "    return df.drop(columns=list(to_drop))\n",
    "\n",
    "def keep_common_active_genes(dfs_dict):\n",
    "    print(\"--- Filtro Geni Comuni Attivi ---\")\n",
    "    \n",
    "    # 1. Trova i geni che hanno almeno un valore != 0 in OGNI vista\n",
    "    valid_genes_per_view = []\n",
    "    \n",
    "    for name, df in dfs_dict.items():\n",
    "        # Calcola la somma per riga (gene)\n",
    "        row_sums = df.sum(axis=1)\n",
    "        # Tieni solo i geni con somma > 0\n",
    "        active_genes = set(row_sums[row_sums > 0].index)\n",
    "        valid_genes_per_view.append(active_genes)\n",
    "        print(f\"   Vista {name}: {len(active_genes)} geni attivi su {len(df)}\")\n",
    "\n",
    "    common_genes = set.intersection(*valid_genes_per_view)\n",
    "    common_genes = sorted(list(common_genes))\n",
    "    \n",
    "    print(f\"   -> Geni validi comuni rimasti: {len(common_genes)}\")\n",
    "    \n",
    "    filtered_dict = {}\n",
    "    for name, df in dfs_dict.items():\n",
    "        # .loc seleziona solo le righe dei geni comuni\n",
    "        filtered_dict[name] = df.loc[common_genes]\n",
    "        \n",
    "    return filtered_dict\n",
    "\n",
    "go_dag = load_ontology(obo_files[\"GO\"][\"path\"])\n",
    "hpo_dag = load_ontology(obo_files[\"HPO\"][\"path\"])\n",
    "\n",
    "processed_dfs = {}\n",
    "\n",
    "for key, path in data_files.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        \n",
    "        if key in depth_files:\n",
    "            df = filter_by_depth(df, depth_files[key], min_depth=4)\n",
    "        df = frequency_filtering(df)\n",
    "        \n",
    "        current_dag = hpo_dag if key == \"HPO\" else go_dag\n",
    "        \n",
    "        df = remove_semantic_redundancy(df, current_dag, threshold=0.7)\n",
    "        \n",
    "        processed_dfs[key] = df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File non trovato: {path}\")\n",
    "\n",
    "if processed_dfs:\n",
    "    final_dfs = keep_common_active_genes(processed_dfs)\n",
    "    print(\"\\n=== Salvataggio ===\")\n",
    "    for key, df in final_dfs.items():\n",
    "        out_name = f\"filtered_final_{key}.csv\"\n",
    "        df.to_csv(out_name)\n",
    "        print(f\"   -> {key}: {df.shape} salvato in {out_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614912b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = {\n",
    "    \"BP\": \"filtered_final_BP.csv\",\n",
    "    \"CC\": \"filtered_final_CC.csv\",\n",
    "    \"MF\": \"filtered_final_MF.csv\",\n",
    "    \"HPO\": \"filtered_final_HPO.csv\"\n",
    "}\n",
    "\n",
    "def process_phase2(file_path, view_name):\n",
    "    print(f\"\\n--- Elaborazione Vista: {view_name} ---\")\n",
    "    \n",
    "    # 1. Caricamento Dati (Matrice Binaria)\n",
    "    try:\n",
    "        df_binary = pd.read_csv(file_path, index_col=0)\n",
    "        print(f\"   Input (Binario): {df_binary.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ERRORE: File {file_path} non trovato. Salto.\")\n",
    "        return\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "    \n",
    "    tfidf_matrix_sparse = tfidf_transformer.fit_transform(df_binary)\n",
    "    \n",
    "    df_tfidf = pd.DataFrame(\n",
    "        tfidf_matrix_sparse.toarray(), \n",
    "        index=df_binary.index, \n",
    "        columns=df_binary.columns\n",
    "    )\n",
    "    print(f\"   TF-IDF Calcolato. Range valori: [{df_tfidf.values.min():.4f}, {df_tfidf.values.max():.4f}]\")\n",
    "\n",
    "    # 3. Output per MOFA (Gene x Termini, Pesato)\n",
    "    mofa_filename = f\"mofa_input_{view_name}.csv\"\n",
    "    df_tfidf.to_csv(mofa_filename)\n",
    "\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix_sparse)\n",
    "    \n",
    "    # Ricostruzione DataFrame simmetrico Gene x Gene\n",
    "    df_sim = pd.DataFrame(\n",
    "        cosine_sim,\n",
    "        index=df_binary.index,\n",
    "        columns=df_binary.index\n",
    "    )\n",
    "    \n",
    "    # Check integrit√† (Diagonale deve essere 1.0)\n",
    "    diag_mean = np.diag(df_sim).mean()\n",
    "    print(f\"   Similarit√† Calcolata: {df_sim.shape}. Media diagonale: {diag_mean:.2f} (atteso 1.0)\")\n",
    "\n",
    "    snf_filename = f\"snf_similarity_{view_name}.csv\"\n",
    "    df_sim.to_csv(snf_filename)\n",
    "\n",
    "for key, filename in input_files.items():\n",
    "    process_phase2(filename, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_view(sim_matrix_path, view_name):\n",
    "    print(f\"\\n--- Analisi Vista: {view_name} ---\")\n",
    "    \n",
    "    sim_df = pd.read_csv(sim_matrix_path, index_col=0)\n",
    "    \n",
    "    distance_matrix = 1 - sim_df.values\n",
    "    distance_matrix[distance_matrix < 0] = 0\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        min_dist=0.1,\n",
    "        n_components=2,\n",
    "        metric='precomputed',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    embedding = reducer.fit_transform(distance_matrix)\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=30,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    \n",
    "    cluster_labels = clusterer.fit_predict(embedding)\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    print(f\"   -> Trovati {n_clusters} cluster.\")\n",
    "    print(f\"   -> Geni scartati come rumore: {n_noise}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Definizione colori\n",
    "    noise_color = (0.8, 0.8, 0.8)\n",
    "    palette = sns.color_palette('tab20', n_colors=n_clusters)\n",
    "    cluster_colors = [palette[x] if x >= 0 else noise_color for x in cluster_labels]\n",
    "    \n",
    "    plt.scatter(\n",
    "        embedding[:, 0], \n",
    "        embedding[:, 1], \n",
    "        c=cluster_colors, \n",
    "        s=5, \n",
    "        alpha=0.6\n",
    "    )\n",
    "    \n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Noise', \n",
    "               markerfacecolor=noise_color, markersize=10),\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.title(f'UMAP Projection - {view_name} ({n_clusters} clusters)', fontsize=16)\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    \n",
    "    plt.savefig(f\"plot_umap_{view_name}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Gene': sim_df.index,\n",
    "        'Cluster': cluster_labels,\n",
    "        'UMAP_1': embedding[:, 0],\n",
    "        'UMAP_2': embedding[:, 1]\n",
    "    })\n",
    "    results.to_csv(f\"clusters_{view_name}.csv\", index=False)\n",
    "    \n",
    "    return n_clusters, n_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dadb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    \"BP\": \"snf_similarity_BP.csv\",\n",
    "    \"CC\": \"snf_similarity_CC.csv\",\n",
    "    \"MF\": \"snf_similarity_MF.csv\",\n",
    "    \"HPO\": \"snf_similarity_HPO.csv\"\n",
    "}\n",
    "\n",
    "summary = []\n",
    "for name, path in files.items():\n",
    "    n_clust, n_noise = analyze_view(path, name)\n",
    "    summary.append({'View': name, 'Clusters': n_clust, 'Noise_Genes': n_noise})\n",
    "\n",
    "print(pd.DataFrame(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0622268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snf_integration(view_names, output_dir='.'):\n",
    "    \"\"\"\n",
    "    Carica le matrici di similarit√† e le fonde usando Similarity Network Fusion (SNF).\n",
    "    \"\"\"\n",
    "    print(\"--- Fase 3: Similarity Network Fusion (SNF) ---\")\n",
    "    \n",
    "    similarity_matrices = []\n",
    "    gene_list = None\n",
    "    \n",
    "    # 1. Caricamento e Verifica delle Matrici\n",
    "    for name in view_names:\n",
    "        filename = f\"snf_similarity_{name}.csv\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"ERRORE: File {filename} non trovato. Esegui la Fase 2 (Cella 13).\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        \n",
    "        # Inizializza l'elenco dei geni con la prima vista\n",
    "        if gene_list is None:\n",
    "            gene_list = df.index.tolist()\n",
    "        \n",
    "        # Verifica che gli indici dei geni siano coerenti tra le viste\n",
    "        if df.index.tolist() != gene_list:\n",
    "             # Questo non dovrebbe succedere se la Fase 1 √® corretta, ma √® una buona prassi\n",
    "            print(f\"ATTENZIONE: Indici dei geni non consistenti per {name}. Ignoro.\")\n",
    "            continue\n",
    "            \n",
    "        similarity_matrices.append(df.values)\n",
    "        print(f\"   Caricato {name}: {df.shape}\")\n",
    "\n",
    "    if not similarity_matrices:\n",
    "        print(\"Nessuna matrice di similarit√† valida trovata. SNF annullato.\")\n",
    "        return\n",
    "\n",
    "    # 2. Esecuzione SNF\n",
    "    # Parametri raccomandati: K=numero di vicini, t=numero di iterazioni\n",
    "    K = 20  # Numero di vicini nel grafo (tipico: 10-30)\n",
    "    t = 20  # Numero di iterazioni (tipico: 10-50)\n",
    "    \n",
    "    print(f\"   Esecuzione SNF con K={K}, t={t} su {len(similarity_matrices)} matrici...\")\n",
    "    \n",
    "    # snf.snf accetta una lista di array numpy\n",
    "    W_fused = snf(similarity_matrices, K=K, t=t)\n",
    "    \n",
    "    print(\"   SNF completato.\")\n",
    "    \n",
    "    # 3. Salvataggio della Matrice Fusa\n",
    "    df_fused = pd.DataFrame(W_fused, index=gene_list, columns=gene_list)\n",
    "    output_file = os.path.join(output_dir, \"snf_fused_similarity_matrix.csv\")\n",
    "    df_fused.to_csv(output_file)\n",
    "    \n",
    "    print(f\"   Matrice Fusa (W_fused) salvata: {df_fused.shape} in {output_file}\")\n",
    "    \n",
    "    return df_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_names = [\"BP\", \"CC\", \"MF\", \"HPO\"]\n",
    "df_fused_snf = run_snf_integration(view_names)\n",
    "\n",
    "if df_fused_snf is not None:\n",
    "    # Controlla la distribuzione dei valori fusi\n",
    "    print(f\"\\nRange Matrice Fusa: [{df_fused_snf.values.min():.4f}, {df_fused_snf.values.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac95e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fase 3: Simulazione MOFA+ Factors (Z_MOFA) ---\n",
      "   Caricati 3317 geni validi.\n",
      "   Simulazione completata. Matrice Z_MOFA (3317x15) salvata in .\\mofa_latent_factors.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor_1</th>\n",
       "      <th>Factor_2</th>\n",
       "      <th>Factor_3</th>\n",
       "      <th>Factor_4</th>\n",
       "      <th>Factor_5</th>\n",
       "      <th>Factor_6</th>\n",
       "      <th>Factor_7</th>\n",
       "      <th>Factor_8</th>\n",
       "      <th>Factor_9</th>\n",
       "      <th>Factor_10</th>\n",
       "      <th>Factor_11</th>\n",
       "      <th>Factor_12</th>\n",
       "      <th>Factor_13</th>\n",
       "      <th>Factor_14</th>\n",
       "      <th>Factor_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.109556</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>0.143314</td>\n",
       "      <td>0.339037</td>\n",
       "      <td>-0.053863</td>\n",
       "      <td>-0.053859</td>\n",
       "      <td>0.351599</td>\n",
       "      <td>0.170089</td>\n",
       "      <td>-0.106480</td>\n",
       "      <td>0.119807</td>\n",
       "      <td>-0.105126</td>\n",
       "      <td>-0.105643</td>\n",
       "      <td>0.052595</td>\n",
       "      <td>-0.429310</td>\n",
       "      <td>-0.387192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.127233</td>\n",
       "      <td>-0.227972</td>\n",
       "      <td>0.068757</td>\n",
       "      <td>-0.204538</td>\n",
       "      <td>-0.317293</td>\n",
       "      <td>0.326207</td>\n",
       "      <td>-0.051990</td>\n",
       "      <td>0.013592</td>\n",
       "      <td>-0.320076</td>\n",
       "      <td>-0.123229</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>-0.258865</td>\n",
       "      <td>0.082498</td>\n",
       "      <td>-0.135808</td>\n",
       "      <td>-0.066729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.136047</td>\n",
       "      <td>0.412656</td>\n",
       "      <td>-0.004525</td>\n",
       "      <td>-0.238007</td>\n",
       "      <td>0.182411</td>\n",
       "      <td>-0.274483</td>\n",
       "      <td>0.045194</td>\n",
       "      <td>-0.439682</td>\n",
       "      <td>-0.298485</td>\n",
       "      <td>0.042510</td>\n",
       "      <td>0.163611</td>\n",
       "      <td>0.036810</td>\n",
       "      <td>-0.027366</td>\n",
       "      <td>-0.068833</td>\n",
       "      <td>-0.332099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.162462</td>\n",
       "      <td>-0.104504</td>\n",
       "      <td>0.234862</td>\n",
       "      <td>0.075325</td>\n",
       "      <td>-0.395717</td>\n",
       "      <td>0.070957</td>\n",
       "      <td>-0.087610</td>\n",
       "      <td>-0.152864</td>\n",
       "      <td>0.135262</td>\n",
       "      <td>0.229021</td>\n",
       "      <td>0.206724</td>\n",
       "      <td>-0.189153</td>\n",
       "      <td>-0.070646</td>\n",
       "      <td>0.072562</td>\n",
       "      <td>0.216621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.108649</td>\n",
       "      <td>-0.043020</td>\n",
       "      <td>-0.248880</td>\n",
       "      <td>-0.268975</td>\n",
       "      <td>0.180171</td>\n",
       "      <td>0.301743</td>\n",
       "      <td>-0.017608</td>\n",
       "      <td>0.222879</td>\n",
       "      <td>0.079353</td>\n",
       "      <td>-0.145754</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.342392</td>\n",
       "      <td>-0.009518</td>\n",
       "      <td>0.348342</td>\n",
       "      <td>-0.587273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729230</th>\n",
       "      <td>-0.161074</td>\n",
       "      <td>0.104320</td>\n",
       "      <td>-0.304503</td>\n",
       "      <td>-0.055763</td>\n",
       "      <td>0.089172</td>\n",
       "      <td>0.028122</td>\n",
       "      <td>-0.165878</td>\n",
       "      <td>0.162847</td>\n",
       "      <td>0.057185</td>\n",
       "      <td>-0.468599</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>-0.095909</td>\n",
       "      <td>-0.118887</td>\n",
       "      <td>-0.187075</td>\n",
       "      <td>-0.225780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790955</th>\n",
       "      <td>0.044684</td>\n",
       "      <td>0.120582</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>0.210787</td>\n",
       "      <td>0.128493</td>\n",
       "      <td>-0.450257</td>\n",
       "      <td>-0.206438</td>\n",
       "      <td>0.264219</td>\n",
       "      <td>-0.285337</td>\n",
       "      <td>-0.613049</td>\n",
       "      <td>0.407999</td>\n",
       "      <td>0.064814</td>\n",
       "      <td>-0.154236</td>\n",
       "      <td>-0.191206</td>\n",
       "      <td>0.139226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100127206</th>\n",
       "      <td>0.105372</td>\n",
       "      <td>0.071654</td>\n",
       "      <td>-0.043711</td>\n",
       "      <td>-0.287841</td>\n",
       "      <td>-0.009549</td>\n",
       "      <td>0.236656</td>\n",
       "      <td>-0.122685</td>\n",
       "      <td>0.132890</td>\n",
       "      <td>-0.091981</td>\n",
       "      <td>0.177648</td>\n",
       "      <td>-0.220620</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.100559</td>\n",
       "      <td>-0.152535</td>\n",
       "      <td>0.079572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100134444</th>\n",
       "      <td>-0.008115</td>\n",
       "      <td>-0.011380</td>\n",
       "      <td>-0.153050</td>\n",
       "      <td>0.221495</td>\n",
       "      <td>-0.212835</td>\n",
       "      <td>0.061047</td>\n",
       "      <td>-0.191387</td>\n",
       "      <td>-0.016303</td>\n",
       "      <td>0.142235</td>\n",
       "      <td>-0.066093</td>\n",
       "      <td>-0.689333</td>\n",
       "      <td>-0.302176</td>\n",
       "      <td>-0.484028</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>-0.002628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100288687</th>\n",
       "      <td>-0.079905</td>\n",
       "      <td>-0.141580</td>\n",
       "      <td>0.047937</td>\n",
       "      <td>-0.325122</td>\n",
       "      <td>0.212264</td>\n",
       "      <td>0.015289</td>\n",
       "      <td>-0.212760</td>\n",
       "      <td>-0.095064</td>\n",
       "      <td>-0.030873</td>\n",
       "      <td>-0.310246</td>\n",
       "      <td>0.148499</td>\n",
       "      <td>0.214369</td>\n",
       "      <td>0.024746</td>\n",
       "      <td>0.208796</td>\n",
       "      <td>0.591025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3317 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Factor_1  Factor_2  Factor_3  Factor_4  Factor_5  Factor_6  \\\n",
       "16         0.109556 -0.032422  0.143314  0.339037 -0.053863 -0.053859   \n",
       "18        -0.127233 -0.227972  0.068757 -0.204538 -0.317293  0.326207   \n",
       "19        -0.136047  0.412656 -0.004525 -0.238007  0.182411 -0.274483   \n",
       "20        -0.162462 -0.104504  0.234862  0.075325 -0.395717  0.070957   \n",
       "21        -0.108649 -0.043020 -0.248880 -0.268975  0.180171  0.301743   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "729230    -0.161074  0.104320 -0.304503 -0.055763  0.089172  0.028122   \n",
       "790955     0.044684  0.120582  0.032742  0.210787  0.128493 -0.450257   \n",
       "100127206  0.105372  0.071654 -0.043711 -0.287841 -0.009549  0.236656   \n",
       "100134444 -0.008115 -0.011380 -0.153050  0.221495 -0.212835  0.061047   \n",
       "100288687 -0.079905 -0.141580  0.047937 -0.325122  0.212264  0.015289   \n",
       "\n",
       "           Factor_7  Factor_8  Factor_9  Factor_10  Factor_11  Factor_12  \\\n",
       "16         0.351599  0.170089 -0.106480   0.119807  -0.105126  -0.105643   \n",
       "18        -0.051990  0.013592 -0.320076  -0.123229   0.023295  -0.258865   \n",
       "19         0.045194 -0.439682 -0.298485   0.042510   0.163611   0.036810   \n",
       "20        -0.087610 -0.152864  0.135262   0.229021   0.206724  -0.189153   \n",
       "21        -0.017608  0.222879  0.079353  -0.145754   0.079300   0.342392   \n",
       "...             ...       ...       ...        ...        ...        ...   \n",
       "729230    -0.165878  0.162847  0.057185  -0.468599   0.100300  -0.095909   \n",
       "790955    -0.206438  0.264219 -0.285337  -0.613049   0.407999   0.064814   \n",
       "100127206 -0.122685  0.132890 -0.091981   0.177648  -0.220620   0.001553   \n",
       "100134444 -0.191387 -0.016303  0.142235  -0.066093  -0.689333  -0.302176   \n",
       "100288687 -0.212760 -0.095064 -0.030873  -0.310246   0.148499   0.214369   \n",
       "\n",
       "           Factor_13  Factor_14  Factor_15  \n",
       "16          0.052595  -0.429310  -0.387192  \n",
       "18          0.082498  -0.135808  -0.066729  \n",
       "19         -0.027366  -0.068833  -0.332099  \n",
       "20         -0.070646   0.072562   0.216621  \n",
       "21         -0.009518   0.348342  -0.587273  \n",
       "...              ...        ...        ...  \n",
       "729230     -0.118887  -0.187075  -0.225780  \n",
       "790955     -0.154236  -0.191206   0.139226  \n",
       "100127206   0.100559  -0.152535   0.079572  \n",
       "100134444  -0.484028   0.013677  -0.002628  \n",
       "100288687   0.024746   0.208796   0.591025  \n",
       "\n",
       "[3317 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_mofa_factors(output_dir='.'):\n",
    "    print(\"\\n--- Fase 3: Simulazione MOFA+ Factors (Z_MOFA) ---\")\n",
    "    \n",
    "    # Tentativo di caricare un file di input per ottenere l'elenco dei geni\n",
    "    try:\n",
    "        df_bp = pd.read_csv(os.path.join(output_dir, \"mofa_input_BP.csv\"), index_col=0)\n",
    "        gene_list = df_bp.index.tolist()\n",
    "        N_GENES = len(gene_list)\n",
    "        print(f\"   Caricati {N_GENES} geni validi.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERRORE: I file 'mofa_input_BP.csv' non sono stati trovati (Eseguire Cella 13).\")\n",
    "        return None\n",
    "\n",
    "    # Numero di Fattori Latenti MOFA che desideriamo estrarre (da Fase 3: 15)\n",
    "    N_FACTORS = 15\n",
    "    \n",
    "    # SIMULAZIONE: Crea una matrice (N_GENES x N_FACTORS) con valori casuali normalizzati.\n",
    "    # Questo simula l'output di un modello MOFA addestrato.\n",
    "    np.random.seed(42) # Per riproducibilit√†\n",
    "    # Genera valori casuali distribuiti normalmente\n",
    "    mofa_factors_data = np.random.randn(N_GENES, N_FACTORS)\n",
    "    \n",
    "    # Normalizzazione per avere una distribuzione sensata\n",
    "    mofa_factors_data = (mofa_factors_data - mofa_factors_data.min()) / (mofa_factors_data.max() - mofa_factors_data.min())\n",
    "    mofa_factors_data = 2 * mofa_factors_data - 1 # Scala tra -1 e 1\n",
    "    \n",
    "    # Crea il DataFrame\n",
    "    factor_names = [f\"Factor_{i+1}\" for i in range(N_FACTORS)]\n",
    "    df_mofa_factors = pd.DataFrame(\n",
    "        mofa_factors_data, \n",
    "        index=gene_list, \n",
    "        columns=factor_names\n",
    "    )\n",
    "    \n",
    "    # Salvataggio dell'Output B\n",
    "    output_file = os.path.join(output_dir, \"mofa_latent_factors.csv\")\n",
    "    df_mofa_factors.to_csv(output_file)\n",
    "    \n",
    "    print(f\"   Simulazione completata. Matrice Z_MOFA ({N_GENES}x{N_FACTORS}) salvata in {output_file}\")\n",
    "    \n",
    "    return df_mofa_factors\n",
    "\n",
    "# Esecuzione della simulazione\n",
    "simulate_mofa_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9efce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SNF e MOFA+ integrati\n",
    "\n",
    "def analyze_dual_integration(sim_matrix_snf_path, mofa_factors_path):\n",
    "    print(f\"\\n--- Fase 4/5: UMAP DUAL (SNF Metric + MOFA Init) ---\")\n",
    "    \n",
    "    # 1. Caricamento Dati\n",
    "    try:\n",
    "        # Carica Matrice di Similarit√† Fusa SNF (per la Metrica)\n",
    "        sim_df = pd.read_csv(sim_matrix_snf_path, index_col=0)\n",
    "        distance_matrix = 1 - sim_df.values\n",
    "        distance_matrix[distance_matrix < 0] = 0 # Assicura distanze non negative\n",
    "        \n",
    "        # Carica Matrice Fattori Latenti MOFA (per l'Inizializzazione)\n",
    "        df_mofa_factors = pd.read_csv(mofa_factors_path, index_col=0)\n",
    "        \n",
    "        # Allinea gli indici e seleziona i Fattori per l'inizializzazione (es. i primi 15)\n",
    "        # Assicurati che i geni siano nello stesso ordine!\n",
    "        initial_embedding = df_mofa_factors.loc[sim_df.index].values\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERRORE di caricamento: {e}. Esegui le celle precedenti.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"ERRORE di allineamento/selezione: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. Riduzione Dimensionale (UMAP)\n",
    "    N_COMPONENTS = 2\n",
    "    initial_embedding = df_mofa_factors.loc[sim_df.index].iloc[:, :N_COMPONENTS].values\n",
    "    \n",
    "    # 2. Riduzione Dimensionale (UMAP)\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        min_dist=0.1,\n",
    "        n_components=N_COMPONENTS,\n",
    "        metric='precomputed', \n",
    "        init=initial_embedding, # Ora di shape (N_GENES, 2)\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"   Esecuzione UMAP (SNF-Guided by MOFA)...\")\n",
    "    # L'input √® la matrice di distanza SNF. L'inizializzazione √® Z_MOFA.\n",
    "    embedding = reducer.fit_transform(distance_matrix)\n",
    "    \n",
    "    # 3. Clustering Robusto (HDBSCAN)\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=30,\n",
    "        metric='euclidean', \n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    \n",
    "    print(\"   Esecuzione HDBSCAN...\")\n",
    "    cluster_labels = clusterer.fit_predict(embedding)\n",
    "    \n",
    "    # 4. Risultati e Statistiche\n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    \n",
    "    print(f\"   -> Risultati Dual: Trovati {n_clusters} cluster.\")\n",
    "    print(f\"   -> Risultati Dual: Geni scartati come rumore: {n_noise} ({n_noise/len(cluster_labels)*100:.2f}%)\")\n",
    "\n",
    "    # 5. Salvataggio\n",
    "    results = pd.DataFrame({\n",
    "        'Gene': sim_df.index,\n",
    "        'Cluster': cluster_labels,\n",
    "        'UMAP_1': embedding[:, 0],\n",
    "        'UMAP_2': embedding[:, 1]\n",
    "    })\n",
    "    results.to_csv(\"clusters_dual_fused_mofa.csv\", index=False)\n",
    "    print(\"   Risultati salvati in clusters_dual_fused_mofa.csv\")\n",
    "    \n",
    "    # La visualizzazione del plot √® omessa qui per brevit√†, ma puoi riaggiungerla con le tue librerie.\n",
    "    \n",
    "    return n_clusters, n_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b054f7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fase 4/5: UMAP DUAL (SNF Metric + MOFA Init) ---\n",
      "   Esecuzione UMAP (SNF-Guided by MOFA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\venv\\lib\\site-packages\\umap\\umap_.py:1865: UserWarning: using precomputed metric; inverse_transform will be unavailable\n",
      "  warn(\"using precomputed metric; inverse_transform will be unavailable\")\n",
      "c:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\venv\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Esecuzione HDBSCAN...\n",
      "   -> Risultati Dual: Trovati 3 cluster.\n",
      "   -> Risultati Dual: Geni scartati come rumore: 28 (0.84%)\n",
      "   Risultati salvati in clusters_dual_fused_mofa.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\husse\\Desktop\\Uni\\ScientificVisualization\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_dual_integration(\n",
    "    sim_matrix_snf_path=\"snf_fused_similarity_matrix.csv\",\n",
    "    mofa_factors_path=\"mofa_latent_factors.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d4364",
   "metadata": {},
   "source": [
    "### ORA BISOGNA CAPIRE PERCH√© i GENI SONO STATI RAGGRUPPATI IN QUESTI 3 CLUSTERS (FASE 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c01a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOLO SNF\n",
    "\n",
    "def analyze_fused_view(sim_matrix_path):\n",
    "    print(f\"\\n--- Fase 4/5: UMAP/HDBSCAN sulla Matrice Fusa ---\")\n",
    "    \n",
    "    # 1. Caricamento Dati\n",
    "    try:\n",
    "        sim_df = pd.read_csv(sim_matrix_path, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRORE: File {sim_matrix_path} non trovato. Esegui la Cella precedente.\")\n",
    "        return\n",
    "    \n",
    "    # Conversione in Matrice di Distanza: D = 1 - S\n",
    "    distance_matrix = 1 - sim_df.values\n",
    "    distance_matrix[distance_matrix < 0] = 0 # Assicura che le distanze siano non negative\n",
    "    \n",
    "    # 2. Riduzione Dimensionale (UMAP)\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        min_dist=0.1,\n",
    "        n_components=2,\n",
    "        metric='precomputed', # Usa la distanza derivata da SNF\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"   Esecuzione UMAP (precomputed)...\")\n",
    "    embedding = reducer.fit_transform(distance_matrix)\n",
    "    \n",
    "    # 3. Clustering Robusto (HDBSCAN)\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=30,\n",
    "        metric='euclidean', # Metrica sull'embedding UMAP\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    \n",
    "    print(\"   Esecuzione HDBSCAN...\")\n",
    "    cluster_labels = clusterer.fit_predict(embedding)\n",
    "    \n",
    "    # 4. Risultati e Statistiche\n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    \n",
    "    print(f\"   -> Risultati Fusi: Trovati {n_clusters} cluster.\")\n",
    "    print(f\"   -> Risultati Fusi: Geni scartati come rumore: {n_noise} ({n_noise/len(cluster_labels)*100:.2f}%)\")\n",
    "\n",
    "    # 5. Visualizzazione e Salvataggio\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    noise_color = (0.8, 0.8, 0.8)\n",
    "    palette = sns.color_palette('tab20', n_colors=n_clusters)\n",
    "    cluster_colors = [palette[x] if x >= 0 else noise_color for x in cluster_labels]\n",
    "    \n",
    "    plt.scatter(\n",
    "        embedding[:, 0], \n",
    "        embedding[:, 1], \n",
    "        c=cluster_colors, \n",
    "        s=5, \n",
    "        alpha=0.6\n",
    "    )\n",
    "    \n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Noise', \n",
    "               markerfacecolor=noise_color, markersize=10),\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.title(f'UMAP Projection - SNF Fused ({n_clusters} clusters)', fontsize=16)\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    \n",
    "    plt.savefig(\"plot_umap_snf_fused.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Gene': sim_df.index,\n",
    "        'Cluster': cluster_labels,\n",
    "        'UMAP_1': embedding[:, 0],\n",
    "        'UMAP_2': embedding[:, 1]\n",
    "    })\n",
    "    results.to_csv(\"clusters_snf_fused.csv\", index=False)\n",
    "    print(\"   Risultati e Plot salvati (clusters_snf_fused.csv e plot_umap_snf_fused.png)\")\n",
    "    \n",
    "    return n_clusters, n_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a468ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_fused_view(\"snf_fused_similarity_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01e0d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pingouin as pg\n",
    "# Nota: pingouin √® spesso usato per Kruskal-Wallis\n",
    "\n",
    "# 1. Caricamento Dati\n",
    "df_clusters = pd.read_csv(\"clusters_dual_fused_mofa.csv\")\n",
    "df_factors = pd.read_csv(\"mofa_latent_factors.csv\", index_col=0) # Contiene 10 colonne (F1 a F10)\n",
    "\n",
    "# Allineamento e rimozione del rumore (-1)\n",
    "df_merged = pd.merge(df_clusters, df_factors, left_on='Gene', right_index=True)\n",
    "df_analysis = df_merged[df_merged['Cluster'] != -1].copy()\n",
    "\n",
    "significant_factors = []\n",
    "\n",
    "# 2. Loop per il Test Statistico (usando Kruskal-Wallis, pi√π robusto)\n",
    "for i in range(1, 11): # Loop sui 10 fattori\n",
    "    factor_col = f'Factor_{i}'\n",
    "    \n",
    "    # Esegue il test Kruskal-Wallis\n",
    "    \n",
    "    aov = pg.kruskal(data=df_analysis, dv=factor_col, between='Cluster')\n",
    "    p_value = aov['p-unc'].iloc[0]\n",
    "        \n",
    "    if p_value < 0.001:\n",
    "        significant_factors.append({\n",
    "            'Factor': factor_col,\n",
    "            'p_value': p_value,\n",
    "            'Status': 'Highly Significant'\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "452dc9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(significant_factors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
